# -*- coding: utf-8 -*-
"""musync.ipynb

Automatically generated by Colaboratory.

Original file is located at
    https://colab.research.google.com/drive/1Tfvfwdo3KP6KcmarFejFKiniNxlIEmI7
"""

!pip3 uninstall googletrans
!pip3 install googletrans==4.0.0-rc1

!pip install Translator

!pip install emoji

!pip install bs4

!pip install SpellChecker

!pip install pandas

!pip install pyspellchecker

from nltk.stem.wordnet import WordNetLemmatizer
from nltk.tag import pos_tag
from nltk.tokenize import word_tokenize
from nltk.tokenize import sent_tokenize
from textblob import TextBlob
from nltk.corpus import stopwords
import re,string
from nltk.sentiment.vader import SentimentIntensityAnalyzer
from bs4 import BeautifulSoup
from spellchecker import SpellChecker
import emoji
import pandas as pd
from googletrans import Translator
from sklearn import datasets, linear_model
from sklearn.model_selection import train_test_split
from matplotlib import pyplot as plt
from nltk.classify.scikitlearn import SklearnClassifier
from sklearn.naive_bayes import MultinomialNB,BernoulliNB
from sklearn.linear_model import LogisticRegression,SGDClassifier
from sklearn.svm import SVC
from sklearn.metrics import f1_score
from nltk.corpus import wordnet
from sklearn.metrics import classification_report

#google translate
translator = Translator(service_urls =['translate.google.com'])

#pyspellchecker
spell = SpellChecker()

data = pd.read_csv("test_data.csv",skip_blank_lines=True,encoding = "latin")
data

l=[]
for i in data["ï»¿Label"]:
    if(i==0):
        l.append("negative")
    else:
        l.append("positive")
data['label']=l

#dropping unwanted columns
data=data.drop(columns=['number', 'date','name','no_query','ï»¿Label'])

data

######
(data.isnull().sum() / len(data))*100

#removes all emojis
def deEmojify(inputString):
    return inputString.encode('ascii', 'ignore').decode('ascii')

# def contractions():

#     return {
#         "ain't":"is not",
#         "amn't":"am not",
#         "aren't":"are not",
#         "can't":"cannot",
#         "'cause":"because",
#         "couldn't":"could not",
#         "couldn't've":"could not have",
#         "could've":"could have",
#         "daren't":"dare not",
#         "daresn't":"dare not",
#         "dasn't":"dare not",
#         "didn't":"did not",
#         "doesn't":"does not",
#         "don't":"do not",
#         "e'er":"ever",
#         "em":"them",
#         "everyone's":"everyone is",
#         "finna":"fixing to",
#         "gimme":"give me",
#         "gonna":"going to",
#         "gon't":"go not",
#         "gotta":"got to",
#         "hadn't":"had not",
#         "hasn't":"has not",
#         "haven't":"have not",
#         "he'd":"he would",
#         "he'll":"he will",
#         "he's":"he is",
#         "he've":"he have",
#         "how'd":"how would",
#         "how'll":"how will",
#         "how're":"how are",
#         "how's":"how is",
#         "ily":"I love you",
#         "Ily":"I love you",
#         "Ihy":"I hate you",
#         "ihy":"I hate you",
#         "imy":"I miss you",
#         "Imy":"I miss you",
#         "I'd":"I would",
#         "I'll":"I will",
#         "I'm":"I am",
#         "im":"I am",
#         "I'm'a":"I am about to",
#         "I'm'o":"I am going to",
#         "isn't":"is not",
#         "it'd":"it would",
#         "it'll":"it will",
#         "it's":"it is",
#         "I've":"I have",
#         "kinda":"kind of",
#         "let's":"let us",
#         "mayn't":"may not",
#         "may've":"may have",
#         "mightn't":"might not",
#         "might've":"might have",
#         "mustn't":"must not",
#         "mustn't've":"must not have",
#         "must've":"must have",
#         "needn't":"need not",
#         "ne'er":"never",
#         "o'":"of",
#         "o'er":"over",
#         "ol'":"old",
#         "oughtn't":"ought not",
#         "shalln't":"shall not",
#         "shan't":"shall not",
#         "she'd":"she would",
#         "she'll":"she will",
#         "she's":"she is",
#         "shouldn't":"should not",
#         "shouldn't've":"should not have",
#         "should've":"should have",
#         "somebody's":"somebody is",
#         "someone's":"someone is",
#         "something's":"something is",
#         "that'd":"that would",
#         "that'll":"that will",
#         "that're":"that are",
#         "that's":"that is",
#         "there'd":"there would",
#         "there'll":"there will",
#         "there're":"there are",
#         "there's":"there is",
#         "these're":"these are",
#         "they'd":"they would",
#         "they'll":"they will",
#         "they're":"they are",
#         "they've":"they have",
#         "this's":"this is",
#         "those're":"those are",
#         "'tis":"it is",
#         "'twas":"it was",
#         "wanna":"want to",
#         "wasn't":"was not",
#         "we'd":"we would",
#         "we'd've":"we would have",
#         "we'll":"we will",
#         "we're":"we are",
#         "weren't":"were not",
#         "we've":"we have",
#         "what'd":"what did",
#         "what'll":"what will",
#         "what're":"what are",
#         "what's":"what is",
#         "what've":"what have",
#         "when's":"when is",
#         "where'd":"where did",
#         "where're":"where are",
#         "where's":"where is",
#         "where've":"where have",
#         "which's":"which is",
#         "who'd":"who would",
#         "who'd've":"who would have",
#         "who'll":"who will",
#         "who're":"who are",
#         "who's":"who is",
#         "who've":"who have",
#         "why'd":"why did",
#         "why're":"why are",
#         "why's":"why is",
#         "won't":"will not",
#         "wouldn't":"would not",
#         "would've":"would have",
#         "y'all":"you all",
#         "you'd":"you would",
#         "you'll":"you will",
#         "you're":"you are",
#         "you've":"you have",
#         "Whatcha":"What are you",
#         "luv":"love",
#         "sux":"sucks",
#         "shit":"bad",
#         "tmr":"tomorrow",
#         "tmrw":"tomorrow",
#         "u":"you",
#         "ur":"your",
#         "k":"okay",
#         "ok":"okay",
#         "da":"the",
#         "tom":"tomorrow",
#         "Tom":"tomorrow",
#         "v'll":"we will",
#         "wassup":"what is up with you",
#         "waddup":"what is up with you",
#         "yo":"greet",
#         "hey":"greet",
#         "lol":"laugh",
#         "lmao":"laugh",
#         "Lmao":"laugh",
#         "rofl":"laugh",
#         "y":"why",
#         "wut":"what",
#         "wat":"what",
#         "stfu":"angry",
#         "wtf":"angry",
#         "ya":"yes",
#         "yeah":"yes",
#         "ummmm":"confused",
#         "ummm":"confused",
#         "umm":"confused",
#         "hmmm":"confused",
#         "i'm":"I am",
#         "awww":"amazement",
#         "Awww":"amazement",
#         "aww":"amazement",
#         "Aww":"amazement",
#         "can't":"cannot",
#         "Can't":"cannot",
#         "CAN'T":"cannot",
#         "awe":"amazement",
#         "Awe":"amazement",
#         "ugh":"sad",
#         "ughh":"sad",
#         "Ugh":"sad",
#         "Ughh":"sad",
#         "UGHH":"sad",
#         "ughhhh":"sad",
#         "ughhh":"sad"
#         }

# def emoticons():

#     return {
#         ":)":"smiley",
#         ":‑)":"smiley",
#         ":-]":"smiley",
#         ":-3":"smiley",
#         ":->":"smiley",
#         "8-)":"smiley",
#         ":-}":"smiley",
#         ":)":"smiley",
#         ":]":"smiley",
#         ":3":"smiley",
#         ":>":"smiley",
#         "8)":"smiley",
#         ":}":"smiley",
#         ":o)":"smiley",
#         ":c)":"smiley",
#         ":^)":"smiley",
#         "=]":"smiley",
#         "=)":"smiley",
#         ":-))":"smiley",
#         ":‑D":"smiley",
#         "8‑D":"smiley",
#         "x‑D":"smiley",
#         "X‑D":"smiley",
#         ":D":"smiley",
#         "8D":"smiley",
#         "xD":"smiley",
#         "XD":"smiley",
#         ":‑(":"sad",
#         ":‑c":"sad",
#         ":‑<":"sad",
#         ":‑[":"sad",
#         ":(":"sad",
#         ":c":"sad",
#         ":<":"sad",
#         ":[":"sad",
#         ":-||":"sad",
#         ">:[":"sad",
#         ":{":"sad",
#         ":@":"sad",
#         ">:(":"sad",
#         ":'‑(":"sad",
#         ":'(":"sad",
#         ":((((":"sad",
#         ":(((":"sad",
#         ":((":"sad",
#         ":(":"sad",
#         ":/":"sad",
#         ":///":"sad",
#         ":////":"sad",
#         "://///":"sad",
#         "://":"sad",
#         ":///////":"sad",
#         ":////":"sad",
#         "-_-":"angry",
#         ":|":"normal",
#         ";)":"playful",
#         ";D":"playful",
#         ":‑P":"playful",
#         "X‑P":"playful",
#         "x‑p":"playful",
#         ":‑p":"playful",
#         ":‑Þ":"playful",
#         ":‑þ":"playful",
#         ":‑b":"playful",
#         ":P":"playful",
#         "XP":"playful",
#         "xp":"playful",
#         ":p":"playful",
#         ":Þ":"playful",
#         ":þ":"playful",
#         ":b":"playful",
#         "<3":"love"
#         }

########
abbreviations = {
    "$" : " dollar ",
    "€" : " euro ",
    "4ao" : "for adults only",
    "a.m" : "before midday",
    "a3" : "anytime anywhere anyplace",
    "aamof" : "as a matter of fact",
    "acct" : "account",
    "adih" : "another day in hell",
    "afaic" : "as far as i am concerned",
    "afaict" : "as far as i can tell",
    "afaik" : "as far as i know",
    "afair" : "as far as i remember",
    "afk" : "away from keyboard",
    "app" : "application",
    "approx" : "approximately",
    "apps" : "applications",
    "asap" : "as soon as possible",
    "asl" : "age, sex, location",
    "atk" : "at the keyboard",
    "ave." : "avenue",
    "aymm" : "are you my mother",
    "ayor" : "at your own risk",
    "b&b" : "bed and breakfast",
    "b+b" : "bed and breakfast",
    "b.c" : "before christ",
    "b2b" : "business to business",
    "b2c" : "business to customer",
    "b4" : "before",
    "b4n" : "bye for now",
    "b@u" : "back at you",
    "bae" : "before anyone else",
    "bak" : "back at keyboard",
    "bbbg" : "bye bye be good",
    "bbc" : "british broadcasting corporation",
    "bbias" : "be back in a second",
    "bbl" : "be back later",
    "bbs" : "be back soon",
    "be4" : "before",
    "bfn" : "bye for now",
    "blvd" : "boulevard",
    "bout" : "about",
    "brb" : "be right back",
    "bros" : "brothers",
     "brt" : "be right there",
    "bsaaw" : "big smile and a wink",
    "btw" : "by the way",
    "bwl" : "bursting with laughter",
    "c/o" : "care of",
    "cet" : "central european time",
    "cf" : "compare",
    "cia" : "central intelligence agency",
    "csl" : "can not stop laughing",
    "cu" : "see you",
    "cul8r" : "see you later",
    "cv" : "curriculum vitae",
    "cwot" : "complete waste of time",
    "cya" : "see you",
    "cyt" : "see you tomorrow",
    "dae" : "does anyone else",
    "dbmib" : "do not bother me i am busy",
    "diy" : "do it yourself",
    "dm" : "direct message",
    "dwh" : "during work hours",
    "e123" : "easy as one two three",
    "eet" : "eastern european time",
     "eg" : "example",
    "embm" : "early morning business meeting",
    "encl" : "enclosed",
    "encl." : "enclosed",
    "etc" : "and so on",
    "faq" : "frequently asked questions",
    "fawc" : "for anyone who cares",
    "fb" : "facebook",
    "fc" : "fingers crossed",
    "fig" : "figure",
    "fimh" : "forever in my heart",
    "ft." : "feet",
    "ft" : "featuring",
    "ftl" : "for the loss",
    "ftw" : "for the win",
    "fwiw" : "for what it is worth",
    "fyi" : "for your information",
    "g9" : "genius",
    "gahoy" : "get a hold of yourself",
    "gal" : "get a life",
    "gcse" : "general certificate of secondary education",
    "gfn" : "gone for now",
    "gg" : "good game",
    "gl" : "good luck",
    "glhf" : "good luck have fun",
    "gmt" : "greenwich mean time",
    "gmta" : "great minds think alike",
    "gn" : "good night",
    "g.o.a.t" : "greatest of all time",
    "goat" : "greatest of all time",
    "goi" : "get over it",
    "gps" : "global positioning system",
    "gr8" : "great",
    "gratz" : "congratulations",
    "gyal" : "girl",
    "h&c" : "hot and cold",
    "hp" : "horsepower",
    "hr" : "hour",
    "hrh" : "his royal highness",
    "ht" : "height",
    "ibrb" : "i will be right back",
    "ic" : "i see",
    "icq" : "i seek you",
    "icymi" : "in case you missed it",
    "idc" : "i do not care",
    "idgadf" : "i do not give a damn fuck",
    "idgaf" : "i do not give a fuck",
    "idk" : "i do not know",
    "ie" : "that is",
    "i.e" : "that is",
    "ifyp" : "i feel your pain",
    "IG" : "instagram",
    "iirc" : "if i remember correctly",
    "ilu" : "i love you",
    "ily" : "i love you",
    "imho" : "in my humble opinion",
    "imo" : "in my opinion",
    "imu" : "i miss you",
    "iow" : "in other words",
    "irl" : "in real life",
    "j4f" : "just for fun",
    "jic" : "just in case",
    "jk" : "just kidding",
    "jsyk" : "just so you know",
    "l8r" : "later",
    "lb" : "pound",
    "lbs" : "pounds",
    "ldr" : "long distance relationship",
    "lmao" : "laugh my ass off",
    "lmfao" : "laugh my fucking ass off",
    "lol" : "laughing out loud",
    "ltd" : "limited",
    "ltns" : "long time no see",
    "m8" : "mate",
    "mf" : "motherfucker",
    "mfs" : "motherfuckers",
    "mfw" : "my face when",
    "mofo" : "motherfucker",
    "mph" : "miles per hour",
    "mr" : "mister",
    "mrw" : "my reaction when",
    "ms" : "miss",
    "mte" : "my thoughts exactly",
    "nagi" : "not a good idea",
    "nbc" : "national broadcasting company",
    "nbd" : "not big deal",
    "nfs" : "not for sale",
    "ngl" : "not going to lie",
    "nhs" : "national health service",
    "nrn" : "no reply necessary",
    "nsfl" : "not safe for life",
    "nsfw" : "not safe for work",
    "nth" : "nice to have",
    "nvr" : "never",
    "nyc" : "new york city",
    "oc" : "original content",
    "og" : "original",
    "ohp" : "overhead projector",
    "oic" : "oh i see",
    "omdb" : "over my dead body",
    "omg" : "oh my god",
    "omw" : "on my way",
    "p.a" : "per annum",
    "p.m" : "after midday",
    "pm" : "prime minister",
    "poc" : "people of color",
    "pov" : "point of view",
    "pp" : "pages",
    "ppl" : "people",
    "prw" : "parents are watching",
    "ps" : "postscript",
    "pt" : "point",
    "ptb" : "please text back",
     "pto" : "please turn over",
    "qpsa" : "what happens",
    "ratchet" : "rude",
    "rbtl" : "read between the lines",
    "rlrt" : "real life retweet",
    "rofl" : "rolling on the floor laughing",
    "roflol" : "rolling on the floor laughing out loud",
    "rotflmao" : "rolling on the floor laughing my ass off",
    "rt" : "retweet",
    "ruok" : "are you ok",
    "sfw" : "safe for work",
     "sk8" : "skate",
    "smh" : "shake my head",
    "sq" : "square",
    "srsly" : "seriously",
    "ssdd" : "same stuff different day",
    "tbh" : "to be honest",
    "tbs" : "tablespooful",
    "tbsp" : "tablespooful",
    "tfw" : "that feeling when",
    "thks" : "thank you",
    "tho" : "though",
    "thx" : "thank you",
     "tia" : "thanks in advance",
    "til" : "today i learned",
    "tl;dr" : "too long i did not read",
    "tldr" : "too long i did not read",
    "tmb" : "tweet me back",
    "tntl" : "trying not to laugh",
    "ttyl" : "talk to you later",
    "u" : "you",
    "u2" : "you too",
    "u4e" : "yours for ever",
    "utc" : "coordinated universal time",
    "w/" : "with",
    "w/o" : "without",
    "w8" : "wait",
    "wassup" : "what is up",
    "wb" : "welcome back",
    "wtf" : "what the fuck",
    "wtg" : "way to go",
    "wtpa" : "where the party at",
    "wuf" : "where are you from",
    "wuzup" : "what is up",
    "wywh" : "wish you were here",
    "ygtr" : "you got that right",
    "ynk" : "you never know",
    "zzz" : "sleeping bored and tired"
}

def removal_of_noise(sent):
    clean_sent=[]
    temp_st=""
    list_sent=sent.split(" ")
    c=0
    d=contractions()
    emoji=emoticons()
    for word in list_sent:
        #removal of url
        word = re.sub(r"http\S+", "", word)
        word = re.sub(r"[www.][a-zA-Z0-9_]+[.com]", "", word)
        #removal of account handles '@'
        word = re.sub("(@[A-Za-z0-9_]+)","", word)

        #replacing emoticons with their respective words
        if(word in emoji.keys()):
            word=emoji[word]
        #replacing short form words with their full form
        if(word.lower() in d.keys()):
            word=d[word.lower()]
        if(c==0):
            temp_st=word
        else:
            temp_st=temp_st+" "+word
        c=c+1
    sent=temp_st
    stop_words = set(stopwords.words('english'))
    stop_words.add('is')
    stop_words.remove('not')
    for word in word_tokenize(sent):
        if(word.lower() not in stop_words and word.lower() not in string.punctuation and word!="'" and word!='"' ):
            #print(word)
            word=spell.correction(word.lower())
            word=re.sub("[0-9]+","",word)
            word=re.sub("[.]+"," ",word)
            word=re.sub("[-]+"," ",word)
            word=re.sub("[_]+"," ",word)
            word = re.sub("~"," ", word)
            if(len(word)!=1):
                clean_sent.append(word.lower())
    cleaned_st=""
    for i in clean_sent:
        cleaned_st=cleaned_st+" "+i
    #print(cleaned_st)
    return lemmatization(cleaned_st)

# def lemmatization(sent):
#     lemmatize=WordNetLemmatizer()
#     sentence_after_lemmatization=[]
#     for word,tag in pos_tag(word_tokenize(sent)):
#         if(tag[0:2]=="NN"):
#             pos='n'
#         elif(tag[0:2]=="VB"):
#             pos='v'
#         else:
#             pos='a'
#         lem=lemmatize.lemmatize(word,pos)
#         sentence_after_lemmatization.append(lem)
#     st=""
#     for i in sentence_after_lemmatization:
#         if(i!="be" and i!="is" and len(i)!=1):
#             st=st+" "+i
#     #print("lemi",st)
#     c=0
#     list_text=st.split()
#     flag=0
#     new_st=""
#     for i in list_text:
#         temp=i
#         if(flag==1):
#             flag=0
#             continue
#         if(i=="not" and (c+1)<len(list_text)):
#             for syn in wordnet.synsets(list_text[c+1]):
#                 antonyms=[]
#                 for l in syn.lemmas():
#                     #print(l)
#                     if l.antonyms():
#                         antonyms.append(l.antonyms()[0].name())
#                         #print(antonyms)
#                         temp=antonyms[0]
#                         flag=1
#                         break
#                 if(flag==1):
#                     break
#         new_st=new_st+" "+temp
#         c+=1
#     #print(new_st)
#     return new_st

# #nltk module to get the sentiment polarity
# def sentiment_analysis(sent):
#         sid = SentimentIntensityAnalyzer()
#         #print("-------------------------------------")
#         print(sent)
#         #print("-------------------------------------")
#         ss = sid.polarity_scores(sent)
#         x=ss['pos']
#         y=ss['neg']
#         print(x-y)
#         print("-------------------------------------")
#         return x-y

# def start(text):
#     #removes html tags
#     text =BeautifulSoup(text).get_text()
#     text =text.replace("’","'")
#     new_text=sent_tokenize(text)
#     #print((new_text))
#     result=0
#     new_str=""
#     #removing emojis
#     for i in new_text:
#         j=deEmojify(i)
#         res=removal_of_noise(j)
#         new_str=new_str+" "+res
#     return new_str

import nltk
nltk.download('punkt')
nltk.download('stopwords')
nltk.download('averaged_perceptron_tagger')
nltk.download('wordnet')
nltk.download('omw-1.4')

###########

import nltk
from nltk.stem import WordNetLemmatizer
from nltk.corpus import stopwords
from nltk.tokenize import word_tokenize
#Stop Words: A stop word is a commonly used word (such as “the”, “a”, “an”, “in”)
#that a search engine has been programmed to ignore,
#both when indexing entries for searching and when retrieving them as the result of a search query.
nltk.download('stopwords')
stopword = set(stopwords.words('english'))

print(stopword)

import warnings
warnings.filterwarnings('ignore')
import re
import string
import pickle
urlPattern = r"((http://)[^ ]*|(https://)[^ ]*|( www\.)[^ ]*)"
userPattern = '@[^\s]+'
some = 'amp,today,tomorrow,going,girl'
def process_tweets(tweet):
  # Lower Casing
    tweet = re.sub(r"he's", "he is", tweet)
    tweet = re.sub(r"there's", "there is", tweet)
    tweet = re.sub(r"We're", "We are", tweet)
    tweet = re.sub(r"That's", "That is", tweet)
    tweet = re.sub(r"won't", "will not", tweet)
    tweet = re.sub(r"they're", "they are", tweet)
    tweet = re.sub(r"Can't", "Cannot", tweet)
    tweet = re.sub(r"wasn't", "was not", tweet)
    tweet = re.sub(r"don\x89Ûªt", "do not", tweet)
    tweet = re.sub(r"aren't", "are not", tweet)
    tweet = re.sub(r"isn't", "is not", tweet)
    tweet = re.sub(r"What's", "What is", tweet)
    tweet = re.sub(r"haven't", "have not", tweet)
    tweet = re.sub(r"hasn't", "has not", tweet)
    tweet = re.sub(r"There's", "There is", tweet)
    tweet = re.sub(r"He's", "He is", tweet)
    tweet = re.sub(r"It's", "It is", tweet)
    tweet = re.sub(r"You're", "You are", tweet)
    tweet = re.sub(r"I'M", "I am", tweet)
    tweet = re.sub(r"shouldn't", "should not", tweet)
    tweet = re.sub(r"wouldn't", "would not", tweet)
    tweet = re.sub(r"i'm", "I am", tweet)
    tweet = re.sub(r"I\x89Ûªm", "I am", tweet)
    tweet = re.sub(r"I'm", "I am", tweet)
    tweet = re.sub(r"Isn't", "is not", tweet)
    tweet = re.sub(r"Here's", "Here is", tweet)
    tweet = re.sub(r"you've", "you have", tweet)
    tweet = re.sub(r"you\x89Ûªve", "you have", tweet)
    tweet = re.sub(r"we're", "we are", tweet)
    tweet = re.sub(r"what's", "what is", tweet)
    tweet = re.sub(r"couldn't", "could not", tweet)
    tweet = re.sub(r"we've", "we have", tweet)
    tweet = re.sub(r"it\x89Ûªs", "it is", tweet)
    tweet = re.sub(r"doesn\x89Ûªt", "does not", tweet)
    tweet = re.sub(r"It\x89Ûªs", "It is", tweet)
    tweet = re.sub(r"Here\x89Ûªs", "Here is", tweet)
    tweet = re.sub(r"who's", "who is", tweet)
    tweet = re.sub(r"I\x89Ûªve", "I have", tweet)
    tweet = re.sub(r"y'all", "you all", tweet)
    tweet = re.sub(r"can\x89Ûªt", "cannot", tweet)
    tweet = re.sub(r"would've", "would have", tweet)
    tweet = re.sub(r"it'll", "it will", tweet)
    tweet = re.sub(r"we'll", "we will", tweet)
    tweet = re.sub(r"wouldn\x89Ûªt", "would not", tweet)
    tweet = re.sub(r"We've", "We have", tweet)
    tweet = re.sub(r"he'll", "he will", tweet)
    tweet = re.sub(r"Y'all", "You all", tweet)
    tweet = re.sub(r"Weren't", "Were not", tweet)
    tweet = re.sub(r"Didn't", "Did not", tweet)
    tweet = re.sub(r"they'll", "they will", tweet)
    tweet = re.sub(r"they'd", "they would", tweet)
    tweet = re.sub(r"DON'T", "DO NOT", tweet)
    tweet = re.sub(r"That\x89Ûªs", "That is", tweet)
    tweet = re.sub(r"they've", "they have", tweet)
    tweet = re.sub(r"i'd", "I would", tweet)
    tweet = re.sub(r"should've", "should have", tweet)
    tweet = re.sub(r"You\x89Ûªre", "You are", tweet)
    tweet = re.sub(r"where's", "where is", tweet)
    tweet = re.sub(r"Don\x89Ûªt", "Do not", tweet)
    tweet = re.sub(r"we'd", "we would", tweet)
    tweet = re.sub(r"i'll", "I will", tweet)
    tweet = re.sub(r"weren't", "were not", tweet)
    tweet = re.sub(r"They're", "They are", tweet)
    tweet = re.sub(r"Can\x89Ûªt", "Cannot", tweet)
    tweet = re.sub(r"you\x89Ûªll", "you will", tweet)
    tweet = re.sub(r"I\x89Ûªd", "I would", tweet)
    tweet = re.sub(r"let's", "let us", tweet)
    tweet = re.sub(r"it's", "it is", tweet)
    tweet = re.sub(r"can't", "cannot", tweet)
    tweet = re.sub(r"don't", "do not", tweet)
    tweet = re.sub(r"you're", "you are", tweet)
    tweet = re.sub(r"i've", "I have", tweet)
    tweet = re.sub(r"that's", "that is", tweet)
    tweet = re.sub(r"i'll", "I will", tweet)
    tweet = re.sub(r"doesn't", "does not", tweet)
    tweet = re.sub(r"i'd", "I would", tweet)
    tweet = re.sub(r"didn't", "did not", tweet)
    tweet = re.sub(r"ain't", "am not", tweet)
    tweet = re.sub(r"you'll", "you will", tweet)
    tweet = re.sub(r"I've", "I have", tweet)
    tweet = re.sub(r"Don't", "do not", tweet)
    tweet = re.sub(r"I'll", "I will", tweet)
    tweet = re.sub(r"I'd", "I would", tweet)
    tweet = re.sub(r"Let's", "Let us", tweet)
    tweet = re.sub(r"you'd", "You would", tweet)
    tweet = re.sub(r"It's", "It is", tweet)
    tweet = re.sub(r"Ain't", "am not", tweet)
    tweet = re.sub(r"Haven't", "Have not", tweet)
    tweet = re.sub(r"Could've", "Could have", tweet)
    tweet = re.sub(r"youve", "you have", tweet)
    tweet = re.sub(r"donå«t", "do not", tweet)

    tweet = re.sub(r"some1", "someone", tweet)
    tweet = re.sub(r"yrs", "years", tweet)
    tweet = re.sub(r"hrs", "hours", tweet)
    tweet = re.sub(r"2morow|2moro", "tomorrow", tweet)
    tweet = re.sub(r"2day", "today", tweet)
    tweet = re.sub(r"4got|4gotten", "forget", tweet)
    tweet = re.sub(r"b-day|bday", "b-day", tweet)
    tweet = re.sub(r"mother's", "mother", tweet)
    tweet = re.sub(r"mom's", "mom", tweet)
    tweet = re.sub(r"dad's", "dad", tweet)
    tweet = re.sub(r"hahah|hahaha|hahahaha", "haha", tweet)
    tweet = re.sub(r"lmao|lolz|rofl", "lol", tweet)
    tweet = re.sub(r"thanx|thnx", "thanks", tweet)
    tweet = re.sub(r"goood", "good", tweet)
    tweet = re.sub(r"some1", "someone", tweet)
    tweet = re.sub(r"some1", "someone", tweet)
    tweet = tweet.lower()
    tweet=tweet[1:]
    # Removing all URls
    tweet = re.sub(urlPattern,'',tweet)
    # Removing all @username.
    tweet = re.sub(userPattern,'', tweet)
    #remove some words
    tweet= re.sub(some,'',tweet)
    #Remove punctuations
    tweet = tweet.translate(str.maketrans("","",string.punctuation))
    #tokenizing words
    tokens = word_tokenize(tweet)
    #tokens = [w for w in tokens if len(w)>2]
    #Removing Stop Words
    final_tokens = [w for w in tokens if w not in stopword]
    #reducing a word to its word stem
    wordLemm = WordNetLemmatizer()
    finalwords=[]
    for w in final_tokens:
      if len(w)>1:
        word = wordLemm.lemmatize(w)
        finalwords.append(word)
    return ' '.join(finalwords)



########
abbreviations = {
    "$" : " dollar ",
    "€" : " euro ",
    "4ao" : "for adults only",
    "a.m" : "before midday",
    "a3" : "anytime anywhere anyplace",
    "aamof" : "as a matter of fact",
    "acct" : "account",
    "adih" : "another day in hell",
    "afaic" : "as far as i am concerned",
    "afaict" : "as far as i can tell",
    "afaik" : "as far as i know",
    "afair" : "as far as i remember",
    "afk" : "away from keyboard",
    "app" : "application",
    "approx" : "approximately",
    "apps" : "applications",
    "asap" : "as soon as possible",
    "asl" : "age, sex, location",
    "atk" : "at the keyboard",
    "ave." : "avenue",
    "aymm" : "are you my mother",
    "ayor" : "at your own risk",
    "b&b" : "bed and breakfast",
    "b+b" : "bed and breakfast",
    "b.c" : "before christ",
    "b2b" : "business to business",
    "b2c" : "business to customer",
    "b4" : "before",
    "b4n" : "bye for now",
    "b@u" : "back at you",
    "bae" : "before anyone else",
    "bak" : "back at keyboard",
    "bbbg" : "bye bye be good",
    "bbc" : "british broadcasting corporation",
    "bbias" : "be back in a second",
    "bbl" : "be back later",
    "bbs" : "be back soon",
    "be4" : "before",
    "bfn" : "bye for now",
    "blvd" : "boulevard",
    "bout" : "about",
    "brb" : "be right back",
    "bros" : "brothers",
     "brt" : "be right there",
    "bsaaw" : "big smile and a wink",
    "btw" : "by the way",
    "bwl" : "bursting with laughter",
    "c/o" : "care of",
    "cet" : "central european time",
    "cf" : "compare",
    "cia" : "central intelligence agency",
    "csl" : "can not stop laughing",
    "cu" : "see you",
    "cul8r" : "see you later",
    "cv" : "curriculum vitae",
    "cwot" : "complete waste of time",
    "cya" : "see you",
    "cyt" : "see you tomorrow",
    "dae" : "does anyone else",
    "dbmib" : "do not bother me i am busy",
    "diy" : "do it yourself",
    "dm" : "direct message",
    "dwh" : "during work hours",
    "e123" : "easy as one two three",
    "eet" : "eastern european time",
     "eg" : "example",
    "embm" : "early morning business meeting",
    "encl" : "enclosed",
    "encl." : "enclosed",
    "etc" : "and so on",
    "faq" : "frequently asked questions",
    "fawc" : "for anyone who cares",
    "fb" : "facebook",
    "fc" : "fingers crossed",
    "fig" : "figure",
    "fimh" : "forever in my heart",
    "ft." : "feet",
    "ft" : "featuring",
    "ftl" : "for the loss",
    "ftw" : "for the win",
    "fwiw" : "for what it is worth",
    "fyi" : "for your information",
    "g9" : "genius",
    "gahoy" : "get a hold of yourself",
    "gal" : "get a life",
    "gcse" : "general certificate of secondary education",
    "gfn" : "gone for now",
    "gg" : "good game",
    "gl" : "good luck",
    "glhf" : "good luck have fun",
    "gmt" : "greenwich mean time",
    "gmta" : "great minds think alike",
    "gn" : "good night",
    "g.o.a.t" : "greatest of all time",
    "goat" : "greatest of all time",
    "goi" : "get over it",
    "gps" : "global positioning system",
    "gr8" : "great",
    "gratz" : "congratulations",
    "gyal" : "girl",
    "h&c" : "hot and cold",
    "hp" : "horsepower",
    "hr" : "hour",
    "hrh" : "his royal highness",
    "ht" : "height",
    "ibrb" : "i will be right back",
    "ic" : "i see",
    "icq" : "i seek you",
    "icymi" : "in case you missed it",
    "idc" : "i do not care",
    "idgadf" : "i do not give a damn fuck",
    "idgaf" : "i do not give a fuck",
    "idk" : "i do not know",
    "ie" : "that is",
    "i.e" : "that is",
    "ifyp" : "i feel your pain",
    "IG" : "instagram",
    "iirc" : "if i remember correctly",
    "ilu" : "i love you",
    "ily" : "i love you",
    "imho" : "in my humble opinion",
    "imo" : "in my opinion",
    "imu" : "i miss you",
    "iow" : "in other words",
    "irl" : "in real life",
    "j4f" : "just for fun",
    "jic" : "just in case",
    "jk" : "just kidding",
    "jsyk" : "just so you know",
    "l8r" : "later",
    "lb" : "pound",
    "lbs" : "pounds",
    "ldr" : "long distance relationship",
    "lmao" : "laugh my ass off",
    "lmfao" : "laugh my fucking ass off",
    "lol" : "laughing out loud",
    "ltd" : "limited",
    "ltns" : "long time no see",
    "m8" : "mate",
    "mf" : "motherfucker",
    "mfs" : "motherfuckers",
    "mfw" : "my face when",
    "mofo" : "motherfucker",
    "mph" : "miles per hour",
    "mr" : "mister",
    "mrw" : "my reaction when",
    "ms" : "miss",
    "mte" : "my thoughts exactly",
    "nagi" : "not a good idea",
    "nbc" : "national broadcasting company",
    "nbd" : "not big deal",
    "nfs" : "not for sale",
    "ngl" : "not going to lie",
    "nhs" : "national health service",
    "nrn" : "no reply necessary",
    "nsfl" : "not safe for life",
    "nsfw" : "not safe for work",
    "nth" : "nice to have",
    "nvr" : "never",
    "nyc" : "new york city",
    "oc" : "original content",
    "og" : "original",
    "ohp" : "overhead projector",
    "oic" : "oh i see",
    "omdb" : "over my dead body",
    "omg" : "oh my god",
    "omw" : "on my way",
    "p.a" : "per annum",
    "p.m" : "after midday",
    "pm" : "prime minister",
    "poc" : "people of color",
    "pov" : "point of view",
    "pp" : "pages",
    "ppl" : "people",
    "prw" : "parents are watching",
    "ps" : "postscript",
    "pt" : "point",
    "ptb" : "please text back",
     "pto" : "please turn over",
    "qpsa" : "what happens",
    "ratchet" : "rude",
    "rbtl" : "read between the lines",
    "rlrt" : "real life retweet",
    "rofl" : "rolling on the floor laughing",
    "roflol" : "rolling on the floor laughing out loud",
    "rotflmao" : "rolling on the floor laughing my ass off",
    "rt" : "retweet",
    "ruok" : "are you ok",
    "sfw" : "safe for work",
     "sk8" : "skate",
    "smh" : "shake my head",
    "sq" : "square",
    "srsly" : "seriously",
    "ssdd" : "same stuff different day",
    "tbh" : "to be honest",
    "tbs" : "tablespooful",
    "tbsp" : "tablespooful",
    "tfw" : "that feeling when",
    "thks" : "thank you",
    "tho" : "though",
    "thx" : "thank you",
     "tia" : "thanks in advance",
    "til" : "today i learned",
    "tl;dr" : "too long i did not read",
    "tldr" : "too long i did not read",
    "tmb" : "tweet me back",
    "tntl" : "trying not to laugh",
    "ttyl" : "talk to you later",
    "u" : "you",
    "u2" : "you too",
    "u4e" : "yours for ever",
    "utc" : "coordinated universal time",
    "w/" : "with",
    "w/o" : "without",
    "w8" : "wait",
    "wassup" : "what is up",
    "wb" : "welcome back",
    "wtf" : "what the fuck",
    "wtg" : "way to go",
    "wtpa" : "where the party at",
    "wuf" : "where are you from",
    "wuzup" : "what is up",
    "wywh" : "wish you were here",
    "ygtr" : "you got that right",
    "ynk" : "you never know",
    "zzz" : "sleeping bored and tired"
}



def emoticons():

    return {
        ":)":"smiley",
        ":‑)":"smiley",
        ":-]":"smiley",
        ":-3":"smiley",
        ":->":"smiley",
        "8-)":"smiley",
        ":-}":"smiley",
        ":)":"smiley",
        ":]":"smiley",
        ":3":"smiley",
        ":>":"smiley",
        "8)":"smiley",
        ":}":"smiley",
        ":o)":"smiley",
        ":c)":"smiley",
        ":^)":"smiley",
        "=]":"smiley",
        "=)":"smiley",
        ":-))":"smiley",
        ":‑D":"smiley",
        "8‑D":"smiley",
        "x‑D":"smiley",
        "X‑D":"smiley",
        ":D":"smiley",
        "8D":"smiley",
        "xD":"smiley",
        "XD":"smiley",
        ":‑(":"sad",
        ":‑c":"sad",
        ":‑<":"sad",
        ":‑[":"sad",
        ":(":"sad",
        ":c":"sad",
        ":<":"sad",
        ":[":"sad",
        ":-||":"sad",
        ">:[":"sad",
        ":{":"sad",
        ":@":"sad",
        ">:(":"sad",
        ":'‑(":"sad",
        ":'(":"sad",
        ":((((":"sad",
        ":(((":"sad",
        ":((":"sad",
        ":(":"sad",
        ":/":"sad",
        ":///":"sad",
        ":////":"sad",
        "://///":"sad",
        "://":"sad",
        ":///////":"sad",
        ":////":"sad",
        "-_-":"angry",
        ":|":"normal",
        ";)":"playful",
        ";D":"playful",
        ":‑P":"playful",
        "X‑P":"playful",
        "x‑p":"playful",
        ":‑p":"playful",
        ":‑Þ":"playful",
        ":‑þ":"playful",
        ":‑b":"playful",
        ":P":"playful",
        "XP":"playful",
        "xp":"playful",
        ":p":"playful",
        ":Þ":"playful",
        ":þ":"playful",
        ":b":"playful",
        "<3":"love"
        }

def convert_abbrev_in_text(tweet):
    t=[]
    words=tweet.split()
    t = [abbreviations[w.lower()] if w.lower() in abbreviations.keys() else w for w in words]
    return ' '.join(t)

data['processed_tweets'] = data['Tweet'].apply(lambda x: process_tweets(x))
data['processed_tweets'] = data['processed_tweets'].apply(lambda x: convert_abbrev_in_text(x))
print('Text Preprocessing complete.')
data

# clean_list=[]
# for i in data["Tweet"]:
#     print()
#     print(i)
#     x= i
#     clean_list.append(x)
#     print()
#     print(x)
#     print("-------------------------")

# with open('cleaned_tweet.txt', 'w') as f:
#     for item in clean_list:
#         f.write("%s\n" % item)

# #reading from file cleaned tweets and storing in a cleaned tweets column in the dataframe
# filename = "cleaned_tweet.txt"
# with open(filename) as f:
#     lines = f.read().splitlines()
# lines
# data["cleaned_tweets"]=lines

##############################
tokenized_tweet=data['processed_tweets'].apply(lambda x: x.split())
tokenized_tweet.head(5)

######################
from sklearn.feature_extraction.text import CountVectorizer
from nltk.tokenize import RegexpTokenizer
token = RegexpTokenizer(r'[a-zA-Z0-9]+')
cv = CountVectorizer(stop_words='english',ngram_range = (1,1),tokenizer = token.tokenize)
text_counts = cv.fit_transform(data['processed_tweets'].values.astype('U'))

def Negation(sentence):
  '''
  Input: Tokenized sentence (List of words)
  Output: Tokenized sentence with negation handled (List of words)
  '''
  temp = int(0)
  for i in range(len(sentence)):
      if sentence[i-1] in ['not',"n't"]:
          antonyms = []
          for syn in wordnet.synsets(sentence[i]):
              syns = wordnet.synsets(sentence[i])
              w1 = syns[0].name()
              temp = 0
              for l in syn.lemmas():
                  if l.antonyms():
                      antonyms.append(l.antonyms()[0].name())
              max_dissimilarity = 0
              for ant in antonyms:
                  syns = wordnet.synsets(ant)
                  w2 = syns[0].name()
                  syns = wordnet.synsets(sentence[i])
                  w1 = syns[0].name()
                  word1 = wordnet.synset(w1)
                  word2 = wordnet.synset(w2)
                  if isinstance(word1.wup_similarity(word2), float) or isinstance(word1.wup_similarity(word2), int):
                      temp = 1 - word1.wup_similarity(word2)
                  if temp>max_dissimilarity:
                      max_dissimilarity = temp
                      antonym_max = ant
                      sentence[i] = antonym_max
                      sentence[i-1] = ''
  while '' in sentence:
      sentence.remove('')
  return sentence

data

#reading the adjective file  ########                    iffff
filename = "english-adjectives.txt"
with open(filename) as f:
    lines = f.read().splitlines()
lines
adjectives=lines

#All adjectives words in the file   iffff
all_words=[]
negative=["not"]
for i in data["processed_tweets"]:
    for word in word_tokenize(i):
        if(word in adjectives or word in negative):
        #if(word in adjectives ):
            all_words.append(word)

len(all_words)

#creating a frequency distribution of each adjectives. #########  ifff
import nltk
BagOfWords = nltk.FreqDist(all_words)
BagOfWords
len(BagOfWords)

# listing the  5000 most frequent words ####   iffff
word_features = list(BagOfWords.keys())[:5000]
len(word_features)
#word_features

data

#assigning feature for each row in clean_tweets ######  iffff
new_list=[]
for i in data["processed_tweets"]:
    st=""
    for j in i.split():
        if(j in word_features):
            st=st+" "+j
    new_list.append(st)

data["processed_tweets"]=new_list

data

#Spliting into test data and train data ####
y=data["label"]
x=data.drop('label',axis=1)
x_train,x_test,y_train,y_test=train_test_split(x,y,test_size=0.15)

x_train.shape

#creating test and train data frames
X_train = pd.DataFrame(columns=['Tweet','processed_tweets'])
X_test = pd.DataFrame(columns=['Tweet','processed_tweets'])
Y_train = []
Y_test = []
X_train = X_train.append(x_train)
for i in y_test:
    Y_test.append(i)
for i in y_train:
     Y_train.append(i)
X_test = X_test.append(x_test)

#spliting into train sets for training
training_set=[]
count=0
for i in (X_train["processed_tweets"]):
    training_set.append((i.split(),Y_train[count]))
    count+=1

def list_to_dict(words_list):
    return dict([(word, True) for word in words_list])

training_set_formatted = [(list_to_dict(element[0]), element[1]) for element in training_set]
training_set_formatted

#spliting into test sets for testing
test_set=[]
count=0
for i in (X_test["processed_tweets"]):
    test_set.append((i.split(),Y_test[count]))
    count+=1

def list_to_dict(words_list):
    return dict([(word, True) for word in words_list])


test_set_formatted= [(list_to_dict(element[0]), element[1]) for element in test_set]

from sklearn.metrics import recall_score,precision_score
from sklearn.feature_extraction.text import CountVectorizer
#making a list of classifiers with their names
classifiers=[]
#making a list of classifiers with their accuracy
accuracy=[]

#naive bayes classifier
classifier = nltk.NaiveBayesClassifier.train(training_set_formatted)

ground_truth = [r[1] for r in test_set_formatted]
preds = [classifier.classify(r[0]) for r in test_set_formatted]
#f1score=f1_score(ground_truth, preds, labels = ['negative', 'positive'], average = 'micro')
recallscore=recall_score(ground_truth, preds, labels = ['negative', 'positive'], average = 'macro')
precisionscore=precision_score(ground_truth, preds, labels = ['negative', 'positive'], average = 'weighted')

#accuracy
#print("Original Naive Bayes Algo accuracy percent:", (nltk.classify.accuracy(classifier, training_set_formatted))*100)
print("Original Naive Bayes Algo accuracy percent:", (nltk.classify.accuracy(classifier, test_set_formatted))*100)
# print("F1-score : ",100*f1score)
# print("Recall Score : ",100*recallscore)
# print("Precision Score : ",100*precisionscore)
print()

classifier.show_most_informative_features(15)

classifiers.append([classifier,"naive bayes classifier"])

accuracy.append([(nltk.classify.accuracy(classifier, test_set_formatted))*100,"NB"])

print("Original Naive Bayes\n")
target_names = [ 'positive','negative']
print(classification_report(Y_test, preds, target_names=target_names))

#Multinomail naive bayes
MNB_clf = SklearnClassifier(MultinomialNB())
MNB_clf.train(training_set_formatted)
print("Multinomail naive bayes classifier accuracy percent:", (nltk.classify.accuracy(MNB_clf, test_set_formatted))*100)

ground_truth = [r[1] for r in test_set_formatted]
preds = [MNB_clf.classify(r[0]) for r in test_set_formatted]
f1score=f1_score(ground_truth, preds, labels = ['negative', 'positive'], average = 'micro')
recallscore=recall_score(ground_truth, preds, labels = ['negative', 'positive'], average = 'macro')
precisionscore=precision_score(ground_truth, preds, labels = ['negative', 'positive'], average = 'weighted')

# print("F1-score : ",100*f1score)
# print("Recall Score : ",100*recallscore)
# print("Precision Score : ",100*precisionscore)

accuracy.append([(nltk.classify.accuracy(MNB_clf, test_set_formatted))*100,"MNB"])

classifiers.append([MNB_clf,"Multinomail naive bayes classifier"])

print("Multinomail naive bayes\n")
target_names = [ 'positive','negative']
print(classification_report(Y_test, preds, target_names=target_names))

#Bernouli naive bayes
BNB_clf = SklearnClassifier(BernoulliNB())
BNB_clf.train(training_set_formatted)
print("Bernoulli naive bayes classifier accuracy percent:", (nltk.classify.accuracy(BNB_clf, test_set_formatted))*100)

ground_truth = [r[1] for r in test_set_formatted]
preds = [BNB_clf.classify(r[0]) for r in test_set_formatted]
f1score=f1_score(ground_truth, preds, labels = ['negative', 'positive'], average = 'micro')
recallscore=recall_score(ground_truth, preds, labels = ['negative', 'positive'], average = 'macro')
precisionscore=precision_score(ground_truth, preds, labels = ['negative', 'positive'], average = 'weighted')

# print("F1-score : ",100*f1score)
# print("Recall Score : ",100*recallscore)
# print("Precision Score : ",100*precisionscore)

accuracy.append([(nltk.classify.accuracy(BNB_clf, test_set_formatted))*100,"BNB"])

classifiers.append([BNB_clf,"Bernouli classifier"])

print("Bernouli naive bayes\n")
target_names = [ 'positive','negative']
print(classification_report(Y_test, preds, target_names=target_names))

#Logistic regression
LogReg_clf = SklearnClassifier(LogisticRegression())
LogReg_clf.train(training_set_formatted)
print("Logistic Regression classifier accuracy percent:", (nltk.classify.accuracy(LogReg_clf, test_set_formatted))*100)

ground_truth = [r[1] for r in test_set_formatted]
preds = [LogReg_clf.classify(r[0]) for r in test_set_formatted]
f1score=f1_score(ground_truth, preds, labels = ['negative', 'positive'], average = 'micro')
recallscore=recall_score(ground_truth, preds, labels = ['negative', 'positive'], average = 'macro')
precisionscore=precision_score(ground_truth, preds, labels = ['negative', 'positive'], average = 'weighted')
###
# print("F1-score : ",100*f1score)
# print("Recall Score : ",100*recallscore)
# print("Precision Score : ",100*precisionscore)
###
accuracy.append([(nltk.classify.accuracy(LogReg_clf, test_set_formatted))*100,"LogReg"])


classifiers.append([LogReg_clf,"Bernouli LogisticRegression_classifier"])

print("Logistic regression\n")
target_names = [ 'positive','negative']
print(classification_report(Y_test, preds, target_names=target_names))

#Stochastic Gradient Descent classifier
# SGD_clf = SklearnClassifier(SGDClassifier())
# SGD_clf.train(training_set_formatted)
# print("Stochastic Gradient Descent Classifier_classifier accuracy percent:", (nltk.classify.accuracy(SGD_clf, test_set_formatted))*100)

# ground_truth = [r[1] for r in test_set_formatted]
# preds = [SGD_clf.classify(r[0]) for r in test_set_formatted]
# f1score=f1_score(ground_truth, preds, labels = ['negative', 'positive'], average = 'micro')
# recallscore=recall_score(ground_truth, preds, labels = ['negative', 'positive'], average = 'macro')
# precisionscore=precision_score(ground_truth, preds, labels = ['negative', 'positive'], average = 'weighted')

# # print("F1-score : ",100*f1score)
# # print("Recall Score : ",100*recallscore)
# # print("Precision Score : ",100*precisionscore)

# accuracy.append([(nltk.classify.accuracy(SGD_clf, test_set_formatted))*100,"SGD"])


# classifiers.append([SGD_clf,"SGD classifier"])

# print("Stochastic Gradient Descent\n")
# target_names = [ 'positive','negative']
# print(classification_report(Y_test, preds, target_names=target_names))

#Support vector classifier
# SVC_clf = SklearnClassifier(SVC())
# SVC_clf.train(training_set_formatted)
# print("Support vector classifier accuracy percent:", (nltk.classify.accuracy(SVC_clf, test_set_formatted))*100)

# ground_truth = [r[1] for r in test_set_formatted]
# preds = [SVC_clf.classify(r[0]) for r in test_set_formatted]
# f1score=f1_score(ground_truth, preds, labels = ['negative', 'positive'], average = 'micro')
# recallscore=recall_score(ground_truth, preds, labels = ['negative', 'positive'], average = 'macro')
# precisionscore=precision_score(ground_truth, preds, labels = ['negative', 'positive'], average = 'weighted')

# # print("F1-score : ",100*f1score)
# # print("Recall Score : ",100*recallscore)
# # print("Precision Score : ",100*precisionscore)

# accuracy.append([(nltk.classify.accuracy(SVC_clf, test_set_formatted))*100,"SVC"])

# classifiers.append([SVC_clf,"SVC classifier"])

# print("Support vector classifier\n")
# target_names = [ 'positive','negative']
# print(classification_report(Y_test, preds, target_names=target_names))

#Max Entropy classifier
# from nltk.classify import  MaxentClassifier
# from sklearn.metrics import f1_score, accuracy_score
# from sklearn.metrics import f1_score

# def max_ent(training_set_formatted):
#     numIterations = 100
#     algorithm = nltk.classify.MaxentClassifier.ALGORITHMS[0]
#     classifier = nltk.MaxentClassifier.train(training_set_formatted, algorithm, max_iter=numIterations)
#     classifier.show_most_informative_features(10)
#     return classifier

# maxent_classifier=max_ent(training_set_formatted)


# ground_truth = [r[1] for r in test_set_formatted]

# preds = [maxent_classifier.classify(r[0]) for r in test_set_formatted]

# f1score=f1_score(ground_truth, preds, labels = ['negative', 'positive'], average = 'micro')
# recallscore=recall_score(ground_truth, preds, labels = ['negative', 'positive'], average = 'macro')
# precisionscore=precision_score(ground_truth, preds, labels = ['negative', 'positive'], average = 'weighted')

# print("Accuracy : ",f1_score(ground_truth, preds, labels = ['positive', 'negative'], average = 'micro')*100)
# # print("F1-score : ",100*f1score)
# # print("Recall Score : ",100*recallscore)
# # print("Precision Score : ",100*precisionscore)

# accuracy.append([(nltk.classify.accuracy(SVC_clf, test_set_formatted))*100,"MaxEnt"])


# classifiers.append([maxent_classifier,"Max Entropy classifier"])

# print("Max Entropy\n")
# target_names = [ 'positive','negative']
# print(classification_report(Y_test, preds, target_names=target_names))

from nltk.classify import ClassifierI
from statistics import mode

# Defininig the ensemble model class

class EnsembleClassifier(ClassifierI):

    def __init__(self, *classifiers):
        self._classifiers = classifiers

    # returns the classification based on majority of votes
    def classify(self, features):
        votes = []
        for c in self._classifiers:
            v = c.classify(features)
            votes.append(v)
        return mode(votes)

# Initializing the ensemble classifier

def hybrid(test_set_formatted):
    ensemble_clf = EnsembleClassifier(classifiers[0][0], classifiers[1][0])
    # List of only feature dictionary from the featureset list of tuples
    feature_list = [f[0] for f in test_set_formatted]
    global c
    # Looping over each to classify each review
    ensemble_preds = [ensemble_clf.classify(features) for features in feature_list]
    for i in range(len(ensemble_preds)):
        if(ensemble_preds[i]==Y_test[i]):
            c+=1
    return ensemble_preds
c=0
preds=hybrid(test_set_formatted)
ground_truth = [r[1] for r in test_set_formatted]


f1score=f1_score(ground_truth, preds, labels = ['negative', 'positive'], average = 'micro')
recallscore=recall_score(ground_truth, preds, labels = ['negative', 'positive'], average = 'macro')
precisionscore=precision_score(ground_truth, preds, labels = ['negative', 'positive'], average = 'weighted')
print("Accuracy of hybrid : ",100*c/len(preds))
# print("F1-score : ",100*f1score)
# print("Recall Score : ",100*recallscore)
# print("Precision Score : ",100*precisionscore)

accuracy.append([100*c/len(preds),"Hybrid"])

print("Hybrid\n")
target_names = [ 'positive','negative']
print(classification_report(Y_test, preds, target_names=target_names))

accuracy

from matplotlib import pyplot as plt
import numpy as np

# x-axis values
x=[]
for i in accuracy:
    x.append(i[0])

# Y-axis values
y = []
for i in accuracy:
    y.append(i[1])

# Function to plot
plt.plot(y,x)


# function to show the plot
plt.show()

from sklearn import svm, datasets
from sklearn.model_selection import train_test_split
from sklearn.metrics import confusion_matrix
from sklearn.utils.multiclass import unique_labels



class_names = [ 'positive','negative']



def plot_confusion_matrix(y_true, y_pred, classes,
                          normalize=False,
                          title=None,
                          cmap=plt.cm.Blues):
    """
    This function prints and plots the confusion matrix.
    Normalization can be applied by setting `normalize=True`.
    """
    if not title:
        if normalize:
            title = 'Normalized confusion matrix'
        else:
            title = 'Confusion matrix, without normalization'

    # Compute confusion matrix
    cm = confusion_matrix(y_true, y_pred)
    # Only use the labels that appear in the data
    if normalize:
        cm = cm.astype('float') / cm.sum(axis=1)[:, np.newaxis]
        print("Normalized confusion matrix")
    else:
        print('Confusion matrix, without normalization')

    print(cm)

    fig, ax = plt.subplots()
    im = ax.imshow(cm, interpolation='nearest', cmap=cmap)
    ax.figure.colorbar(im, ax=ax)
    # We want to show all ticks...
    ax.set(xticks=np.arange(cm.shape[1]),
           yticks=np.arange(cm.shape[0]),
           # ... and label them with the respective list entries
           xticklabels=classes, yticklabels=classes,
           title=title,
           ylabel='True label',
           xlabel='Predicted label')

    # Rotate the tick labels and set their alignment.
    plt.setp(ax.get_xticklabels(), rotation=45, ha="right",
             rotation_mode="anchor")

    # Loop over data dimensions and create text annotations.
    fmt = '.2f' if normalize else 'd'
    thresh = cm.max() / 2.
    for i in range(cm.shape[0]):
        for j in range(cm.shape[1]):
            ax.text(j, i, format(cm[i, j], fmt),
                    ha="center", va="center",
                    color="white" if cm[i, j] > thresh else "black")
    fig.tight_layout()
    return ax


np.set_printoptions(precision=2)

# Plot non-normalized confusion matrix
plot_confusion_matrix(Y_test, preds, classes=class_names,
                      title='Confusion matrix, without normalization')

# Plot normalized confusion matrix
plot_confusion_matrix(Y_test, preds, classes=class_names, normalize=True,
                      title='Normalized confusion matrix')

plt.show()

def features(text):
    new_list=[]
    for i in text.split():
        #if(i in adjectives):
            new_list.append(i)
    return new_list

def text_classify(text):
    cleaned_text=(text)
    temp=features(cleaned_text)
    test_data=list_to_dict(temp)
    print(temp)
    print("Tweet given by user : ",text)
    for i in classifiers:
        print(i[1])
        determined_label=i[0].classify(test_data)
        print("This Tweet is ",determined_label)
        print("------------------------------")
    c=0
    print("Hybrid model")
    testset_data=[]
    testset_data.append([test_data,""])
    lab=hybrid(testset_data)
    print("This Tweet is ",lab[0])

text_classify("you are awesome")

# import nltk
  # nltk.download('twitter_samples')

# from nltk.corpus import twitter_samples

# positive_tweets = twitter_samples.strings('positive_tweets.json')
# negative_tweets = twitter_samples.strings('negative_tweets.json')
# x=negative_tweets[:1000]

# st=" "
# for i in x:
#     st=st+" "+i
# st

text_classify("I am sad")

def hinglish(input_text):
    translator = Translator(service_urls =['translate.google.com'])
    x=translator.translate(input_text,src="hi",dest="en")
    text_classify(x.text)

from textblob import TextBlob

def hinglish2(input_text):
    l=input_text.split()
    st=" "
    for i in l:
        word=TextBlob(i)
        if(word.detect_language()=="hi"):
            translator = Translator(service_urls =['translate.google.com'])
            x=translator.translate(i,src="hi",dest="en")
            st=st+" "+x.text
        else:
            st=st+" "+i
    text_classify(st)

def func(input_text):
    l=input_text.split()
    flag=0
    for i in l:
        k=len(i)
        if(k<3):
            flag=1
            hinglish(input_text)
    if(not(flag)):
        hinglish2(input_text)

func("mujhe bhook lg rhi h")

!pip install lightgbm

import pandas as pd
from sklearn.cluster import KMeans
from sklearn.preprocessing import MinMaxScaler
from sklearn.model_selection import train_test_split
from lightgbm import LGBMClassifier

data=pd.read_csv('spotify_music.csv')
data.drop_duplicates(inplace=True,subset=['name','uri'])
name=data['name']
#uri=data['uri']
col_features = ['danceability', 'energy', 'valence', 'loudness']

X = MinMaxScaler().fit_transform(data[col_features])

kmeans = KMeans(init="k-means++",
                n_clusters=2,
                random_state=15).fit(X)
data['kmeans'] = kmeans.labels_
data['song_name']=name
#data['uri'] = uri

data

og_data=data.copy()

# cluster=data.groupby(by=data['kmeans'])
# y=data.pop('kmeans')
# x=data.drop(columns=['name','id','song_name'])

# x_train,x_test,y_train,y_test=train_test_split(x,y,test_size=0.25)
# model=LGBMClassifier().fit(x_train,y_train)
# model.score(x_train,y_train)

# model.score(x_test,y_test)

# import random
# random.shuffle(name)
from sklearn.utils import shuffle
df=cluster.apply(lambda x: x.sort_values(["popularity"], ascending=False))
df.reset_index(level=0, inplace=True)

from sklearn.utils import shuffle

EMOTIONS = ["positive","negative"]

def get_results(emotion_code, NUM_RECOMMEND=10):
  positive_set=[]
  negative_set=[]
  if emotion_code==0:
      positive_set.append(df[df['kmeans']==0]['song_name'].head(NUM_RECOMMEND))
      return pd.DataFrame(positive_set).T
  else:
      negative_set.append(df[df['kmeans']==1]['song_name'].head(NUM_RECOMMEND))
      return pd.DataFrame(negative_set).T

print("Hello I am your bot")
input_text = input("How are you?")
translator = Translator(service_urls =['translate.google.com'])
x=translator.translate(input_text,src="hi",dest="en")
text_classify(x.text)
#text=input("How are you?");#
cleaned_text=(x.text)
temp=features(cleaned_text)
test_data=list_to_dict(temp)
print(temp)
print("Tweet given by user : ",x.text)
for i in classifiers:
        print(i[1])
        determined_label=i[0].classify(test_data)
        print("This Tweet is ",determined_label)
        print("------------------------------")
        c=0
        print("Hybrid model")
        testset_data=[]
        testset_data.append([test_data,""])
        lab=hybrid(testset_data)
        print("This Tweet is ",lab[0])
NUM_RECOMMEND=int(input("Enter number of recommendations: "))
if determined_label=='negative':
    emotion_code=1
else:
    emotion_code=0
results= get_results(emotion_code,NUM_RECOMMEND)
print(results)
a = input("do you like these?")
print(a)
if a=='yes':
  print("Happy to hear this")
elif a=='no':
  print("sorry i dissapointed you")
else:
  print("Sorry i did not understand")